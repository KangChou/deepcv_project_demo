

学习资料





# 机器学习

[从机器学习开始](https://machinelearningmastery.com/start-here/)(machinelearningmastery.com)

[机器学习很有趣！](https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471)(medium.com/@ageitgey)

[机器学习规则：ML 工程的最佳实践](http://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf)(martin.zinkevich.org)

机器学习速成课程：[第一部分](https://ml.berkeley.edu/blog/2016/11/06/tutorial-1/)、[第二部分](https://ml.berkeley.edu/blog/2016/12/24/tutorial-2/)、[第三部分](https://ml.berkeley.edu/blog/2017/02/04/tutorial-3/)（伯克利的机器学习）

[机器学习理论及其应用简介：带有示例的可视化教程](https://www.toptal.com/machine-learning/machine-learning-theory-an-introductory-primer)(toptal.com)

[我应该使用哪种机器学习算法？](https://blogs.sas.com/content/subconsciousmusings/2017/04/12/machine-learning-algorithm-use/)(sas.com)

[机器学习入门](https://www.sas.com/content/dam/SAS/en_us/doc/whitepaper1/machine-learning-primer-108796.pdf)(sas.com)

[初学者机器学习教程](https://www.kaggle.com/kanncaa1/machine-learning-tutorial-for-beginners)(kaggle.com/kanncaa1)

## 激活和损失函数

[Sigmoid 神经元](http://neuralnetworksanddeeplearning.com/chap1.html#sigmoid_neurons)(neuralnetworksanddeeplearning.com)

[激活函数在神经网络中的作用是什么？](https://www.quora.com/What-is-the-role-of-the-activation-function-in-a-neural-network)(quora.com)

[具有优点/缺点的神经网络中的激活函数的综合列表](https://stats.stackexchange.com/questions/115258/comprehensive-list-of-activation-functions-in-neural-networks-with-pros-cons)(stats.stackexchange.com)

[激活函数及其类型——哪个更好？](https://medium.com/towards-data-science/activation-functions-and-its-types-which-is-better-a9a5310cc8f)(medium.com)

[理解对数损失](http://www.exegetic.biz/blog/2015/12/making-sense-logarithmic-loss/)(exegetic.biz)

[损失函数](http://cs231n.github.io/neural-networks-2/#losses)（斯坦福 CS231n）

[L1 与 L2 损失函数](http://rishy.github.io/ml/2015/07/28/l1-vs-l2-loss/)(rishy.github.io)

[交叉熵成本函数](http://neuralnetworksanddeeplearning.com/chap3.html#the_cross-entropy_cost_function)(neuralnetworksanddeeplearning.com)

## 偏差

[偏差在神经网络中的作用](https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks/2499936#2499936)(stackoverflow.com)

[神经网络中的偏置节点](http://makeyourownneuralnetwork.blogspot.com/2016/06/bias-nodes-in-neural-networks.html)(makeyourownneuralnetwork.blogspot.com)

[什么是人工神经网络中的偏差？](https://www.quora.com/What-is-bias-in-artificial-neural-network)(quora.com)

## 感知器

[感知器](http://neuralnetworksanddeeplearning.com/chap1.html#perceptrons)(neuralnetworksanddeeplearning.com)

[感知](http://natureofcode.com/book/chapter-10-neural-networks/#chapter10_figure3)（natureofcode.com）

[单层神经网络（感知器）](http://computing.dcu.ie/~humphrys/Notes/Neural/single.neural.html)（dcu.ie）

[从感知器到深度网络](https://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks)(toptal.com)

## 回归

[线性回归分析简介](http://people.duke.edu/~rnau/regintro.htm)(duke.edu)

[线性回归](http://ufldl.stanford.edu/tutorial/supervised/LinearRegression/)(ufldl.stanford.edu)

[线性回归](http://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html)(readthedocs.io)

[逻辑回归](http://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html)(readthedocs.io)

[机器学习的简单线性回归教程](http://machinelearningmastery.com/simple-linear-regression-tutorial-for-machine-learning/)(machinelearningmastery.com)

[机器学习的逻辑回归教程](http://machinelearningmastery.com/logistic-regression-tutorial-for-machine-learning/)(machinelearningmastery.com)

[Softmax 回归](http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/)(ufldl.stanford.edu)

## 梯度下降

[梯度下降学习](http://neuralnetworksanddeeplearning.com/chap1.html#learning_with_gradient_descent)(neuralnetworksanddeeplearning.com)

[梯度下降](http://iamtrask.github.io/2015/07/27/python-network-part2/)(iamtrask.github.io)

[如何理解梯度下降算法](http://www.kdnuggets.com/2017/04/simple-understand-gradient-descent-algorithm.html)（kdnuggets.com）

[梯度下降优化算法概述](http://sebastianruder.com/optimizing-gradient-descent/)(sebastianruder.com)

[优化：随机梯度下降](http://cs231n.github.io/optimization-1/)(Stanford CS231n)

## 生成学习

[生成学习算法](http://cs229.stanford.edu/notes/cs229-notes2.pdf)（斯坦福 CS229）

[朴素贝叶斯分类器的实用解释](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/)(monkeylearn.com)

## 支持向量机

[支持向量机 (SVM)](https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/)简介 (monkeylearn.com)

[支持向量机](http://cs229.stanford.edu/notes/cs229-notes3.pdf)（斯坦福 CS229）

[线性分类：支持向量机、Softmax](http://cs231n.github.io/linear-classify/) (Stanford 231n)

## 反向传播

[是的，您应该了解反向传播](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)(medium.com/@karpathy)

[你能给神经网络的反向传播算法一个直观的解释吗？](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/visual-backpropagation.md)(github.com/rasbt)

[反向传播算法的工作原理](http://neuralnetworksanddeeplearning.com/chap2.html)（neuralnetworksanddeeplearning.com）

[通过时间和梯度消失](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/)的反向传播(wildml.com)

[时间反向传播的简要介绍](http://machinelearningmastery.com/gentle-introduction-backpropagation-time/)(machinelearningmastery.com)

[反向传播，直觉](http://cs231n.github.io/optimization-2/)（斯坦福 CS231n）

## 深度学习

[YN² 深度学习指南](http://yerevann.com/a-guide-to-deep-learning/)(yerevann.com)

[深度学习论文阅读路线图](https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap)(github.com/floodsung)

[简而言之深度学习](http://nikhilbuduma.com/2014/12/29/deep-learning-in-a-nutshell/)(nikhilbuduma.com)

[深度学习教程](http://ai.stanford.edu/~quocle/tutorial1.pdf)(Quoc V. Le)

[什么是深度学习？](http://machinelearningmastery.com/what-is-deep-learning/)(machinelearningmastery.com)

[人工智能、机器学习和深度学习之间有什么区别？](https://blogs.nvidia.com/blog/2016/07/29/whats-difference-artificial-intelligence-machine-learning-deep-learning-ai/)(nvidia.com)

[深度学习——直接的涂料](https://gluon.mxnet.io/)(gluon.mxnet.io)

## 优化和降维

[七种数据降维技术](https://www.knime.org/blog/seven-techniques-for-data-dimensionality-reduction)(knime.org)

[主成分分析](http://cs229.stanford.edu/notes/cs229-notes10.pdf)（斯坦福 CS229）

[Dropout：一种改进神经网络的简单方法](http://videolectures.net/site/normal_dl/tag=741100/nips2012_hinton_networks_01.pdf)（Hinton @ NIPS 2012）

[如何训练你的深度神经网络](http://rishy.github.io/ml/2017/01/05/how-to-train-your-dnn/)(rishy.github.io)

## 长短期记忆 (LSTM)

[专家对长短期记忆网络的温和介绍](http://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/)(machinelearningmastery.com)

[了解 LSTM 网络](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)(colah.github.io)

[探索 LSTM](http://blog.echen.me/2017/05/30/exploring-lstms/) (echen.me)

[任何人都可以学习用 Python 编写 LSTM-RNN](http://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/) (iamtrask.github.io)

## 卷积神经网络 (CNN)

[介绍卷积网络](http://neuralnetworksanddeeplearning.com/chap6.html#introducing_convolutional_networks)(neuralnetworksanddeeplearning.com)

[深度学习和卷积神经网络](https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721)(medium.com/@ageitgey)

[卷积网络：模块化视角](http://colah.github.io/posts/2014-07-Conv-Nets-Modular/)(colah.github.io)

[理解卷积](http://colah.github.io/posts/2014-07-Understanding-Convolutions/)(colah.github.io)

## 循环神经网络 (RNN)

[循环神经网络教程](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)(wildml.com)

[注意力和增强循环神经网络](http://distill.pub/2016/augmented-rnns/)(distill.pub)

[循环神经网络的不合理有效性](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)（karpathy.github.io）

[深入研究循环神经网络](http://nikhilbuduma.com/2015/01/11/a-deep-dive-into-recurrent-neural-networks/)(nikhilbuduma.com)

## 强化学习

[强化学习及其实施的简单初学者指南](https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/)(analyticsvidhya.com)

[强化学习教程](https://web.mst.edu/~gosavia/tutorial.pdf)(mst.edu)

[学习强化学习](http://www.wildml.com/2016/10/learning-reinforcement-learning/)(wildml.com)

[深度强化学习：Pong from Pixels](http://karpathy.github.io/2016/05/31/rl/) (karpathy.github.io)

## 生成对抗网络 (GAN)

[对抗机器学习](https://aaai18adversarial.github.io/slides/AML.pptx)(aaai18adversarial.github.io)

[什么是生成对抗网络？](https://blogs.nvidia.com/blog/2017/05/17/generative-adversarial-network/)(nvidia.com)

[滥用生成对抗网络制作 8 位像素艺术](https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7)(medium.com/@ageitgey)

[生成对抗网络简介（TensorFlow 中的代码）](http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/)（aylien.com）

[初学者的生成对抗网络](https://www.oreilly.com/learning/generative-adversarial-networks-for-beginners)(oreilly.com)

## 多任务学习

[深度神经网络中的多任务学习概述](http://sebastianruder.com/multi-task/index.html)(sebastianruder.com)

# 自然语言处理

[自然语言处理很有趣！](https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e)(medium.com/@ageitgey)

[自然语言处理的神经网络模型入门](http://u.cs.biu.ac.il/~yogo/nnlp.pdf)(Yoav Goldberg)

[自然语言处理权威指南](https://monkeylearn.com/blog/definitive-guide-natural-language-processing/)(monkeylearn.com)

[自然语言处理简介](https://blog.algorithmia.com/introduction-natural-language-processing-nlp/)(algorithmia.com)

[自然语言处理教程](http://www.vikparuchuri.com/blog/natural-language-processing-tutorial/)(vikparuchuri.com)

[从 Scratch](https://arxiv.org/pdf/1103.0398.pdf) (arxiv.org) 开始的自然语言处理（几乎）

## 深度学习和自然语言处理

[深度学习应用于 NLP](https://arxiv.org/pdf/1703.03091.pdf) (arxiv.org)

[NLP 的深度学习（没有魔法）](https://nlp.stanford.edu/courses/NAACL2013/NAACL2013-Socher-Manning-DeepLearning.pdf)（Richard Socher）

[了解 NLP 的卷积神经网络](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)(wildml.com)

[深度学习、NLP 和表示](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)(colah.github.io)

[嵌入、编码、参与、预测：最先进 NLP 模型的新深度学习公式](https://explosion.ai/blog/deep-learning-formula-nlp)(explosion.ai)

[](https://devblogs.nvidia.com/parallelforall/understanding-natural-language-deep-neural-networks-using-torch/)使用 Torch (nvidia.com)[通过深度神经网络理解自然语言](https://devblogs.nvidia.com/parallelforall/understanding-natural-language-deep-neural-networks-using-torch/)

[使用 Pytorch 进行 NLP 深度学习](http://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html)(pytorich.org)

## 词向量

[词袋遇到爆米花袋](https://www.kaggle.com/c/word2vec-nlp-tutorial)（kaggle.com）

关于词嵌入[Part I](http://sebastianruder.com/word-embeddings-1/index.html) , [Part II](http://sebastianruder.com/word-embeddings-softmax/index.html) , [Part III](http://sebastianruder.com/secret-word2vec/index.html) (sebastianruder.com)

[词向量的惊人力量](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/)(acolyer.org)

[word2vec 参数学习解释](https://arxiv.org/pdf/1411.2738.pdf)(arxiv.org)

Word2Vec 教程 — [Skip-Gram 模型](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)，[负采样](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)(mccormickml.com)

## 编码器-解码器

[深度学习和 NLP 中的注意力和记忆](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/)(wildml.com)

[序列到序列模型](https://www.tensorflow.org/tutorials/seq2seq)(tensorflow.org)

[使用神经网络进行序列到序列学习](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)(NIPS 2014)

[机器学习很有趣第 5 部分：深度学习的语言翻译和序列的魔力](https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa)(medium.com/@ageitgey)

[如何使用编码器-解码器 LSTM 来回显随机整数序列](http://machinelearningmastery.com/how-to-use-an-encoder-decoder-lstm-to-echo-sequences-of-random-integers/)(machinelearningmastery.com)

[tf-seq2seq](https://google.github.io/seq2seq/) (google.github.io)

# Python

[机器学习速成课程](https://developers.google.com/machine-learning/crash-course/)(google.com)

[很棒的机器学习](https://github.com/josephmisiti/awesome-machine-learning#python)(github.com/josephmisiti)

[使用 Python 掌握机器学习的 7 个步骤](http://www.kdnuggets.com/2015/11/seven-steps-machine-learning-python.html)(kdnuggets.com)

[机器学习笔记本示例](http://nbviewer.jupyter.org/github/rhiever/Data-Analysis-and-Machine-Learning-Projects/blob/master/example-data-science-notebook/Example%20Machine%20Learning%20Notebook.ipynb)(nbviewer.jupyter.org)

[使用 Python 进行机器学习](https://www.tutorialspoint.com/machine_learning_with_python/machine_learning_with_python_quick_guide.htm)(tutorialspoint.com)

## 例子

[如何在 Python 中从头开始实现感知器算法](http://machinelearningmastery.com/implement-perceptron-algorithm-scratch-python/)(machinelearningmastery.com)

[在 Python 中从头开始实现神经网络](http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/)(wildml.com)

[11 行 Python 中的神经网络](http://iamtrask.github.io/2015/07/12/basic-python-network/)(iamtrask.github.io)

[使用 Python 实现自己的 k-最近邻算法](http://www.kdnuggets.com/2016/01/implementing-your-own-knn-using-python.html)(kdnuggets.com)

[来自 Scatch 的机器学习](https://github.com/eriklindernoren/ML-From-Scratch)(github.com/eriklindernoren)

[Python 机器学习（第 2 版）代码存储库](https://github.com/rasbt/python-machine-learning-book-2nd-edition)(github.com/rasbt)

## Scipy 和 numpy

[Scipy 讲义](http://www.scipy-lectures.org/)(scipy-lectures.org)

[Python Numpy 教程](http://cs231n.github.io/python-numpy-tutorial/)（斯坦福 CS231n）

[Numpy 和 Scipy 简介](https://engineering.ucsb.edu/~shell/che210d/numpy.pdf)(UCSB CHE210D)

[面向科学家的 Python 速成课程](http://nbviewer.jupyter.org/gist/rpmuller/5920182#ii.-numpy-and-scipy)(nbviewer.jupyter.org)

## scikit-学习

[PyCon scikit-learn 教程索引](http://nbviewer.jupyter.org/github/jakevdp/sklearn_pycon2015/blob/master/notebooks/Index.ipynb)(nbviewer.jupyter.org)

[scikit-learn 分类算法](https://github.com/mmmayo13/scikit-learn-classifiers/blob/master/sklearn-classifiers-tutorial.ipynb)(github.com/mmmayo13)

[scikit-learn 教程](http://scikit-learn.org/stable/tutorial/index.html)(scikit-learn.org)

[scikit-learn 精简教程](https://github.com/mmmayo13/scikit-learn-beginners-tutorials)(github.com/mmmayo13)

## 张量流

[TensorFlow 教程](https://www.tensorflow.org/tutorials/)(tensorflow.org)

[TensorFlow 简介 — CPU 与 GPU](https://medium.com/@erikhallstrm/hello-world-tensorflow-649b15aed18c) (medium.com/@erikhallstrm)

[TensorFlow：入门](https://blog.metaflow.fr/tensorflow-a-primer-4b3fa0978be3)（metaflow.fr）

[Tensorflow 中的 RNN](http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/) (wildml.com)

[在 TensorFlow 中实现用于文本分类的 CNN](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/) (wildml.com)

[如何使用 TensorFlow 运行文本摘要](http://pavel.surmenok.com/2016/10/15/how-to-run-text-summarization-with-tensorflow/)(surmenok.com)

## PyTorch

[PyTorch 教程](http://pytorch.org/tutorials/)(pytorch.org)

[PyTorch 简介](http://blog.gaurav.im/2017/04/24/a-gentle-intro-to-pytorch/)(gaurav.im)

[教程：PyTorch 中的深度学习](https://iamtrask.github.io/2017/01/15/pytorch-tutorial/)(iamtrask.github.io)

[PyTorch 示例](https://github.com/jcjohnson/pytorch-examples)(github.com/jcjohnson)

[PyTorch 教程](https://github.com/MorvanZhou/PyTorch-Tutorial)(github.com/MorvanZhou)

[深度学习研究人员的 PyTorch 教程](https://github.com/yunjey/pytorch-tutorial)(github.com/yunjey)

# 数学

[机器学习数学](https://people.ucsc.edu/~praman1/static/pub/math-for-ml.pdf)(ucsc.edu)

[机器学习数学](http://www.umiacs.umd.edu/~hal/courses/2013S_ML/math4ml.pdf)(UMIACS CMSC422)

## 线性代数

[线性代数直观指南](https://betterexplained.com/articles/linear-algebra-guide/)（betterexplained.com）

[程序员对矩阵乘法的直觉](https://betterexplained.com/articles/matrix-multiplication/)(betterexplained.com)

[了解交叉产品](https://betterexplained.com/articles/cross-product/)（betterexplained.com）

[了解点积](https://betterexplained.com/articles/vector-calculus-understanding-the-dot-product/)(betterexplained.com)

[机器学习的线性代数](http://www.cedar.buffalo.edu/~srihari/CSE574/Chap1/LinearAlgebra.pdf)（布法罗大学 CSE574）

[用于深度学习的线性代数备忘单](https://medium.com/towards-data-science/linear-algebra-cheat-sheet-for-deep-learning-cd67aba4526c)(medium.com)

[线性代数复习和参考](http://cs229.stanford.edu/section/cs229-linalg.pdf)（斯坦福 CS229）

## 概率

[用比率理解贝叶斯定理](https://betterexplained.com/articles/understanding-bayes-theorem-with-ratios/)(betterexplained.com)

[概率论回顾](http://cs229.stanford.edu/section/cs229-prob.pdf)（斯坦福 CS229）

[机器学习的概率论评论](https://see.stanford.edu/materials/aimlcs229/cs229-prob.pdf)（斯坦福 CS229）

[概率论](http://www.cedar.buffalo.edu/~srihari/CSE574/Chap1/Probability-Theory.pdf)（布法罗大学 CSE574）

[机器学习的概率论](http://www.cs.toronto.edu/~urtasun/courses/CSC411_Fall16/tutorial1.pdf)（多伦多大学 CSC411）

## 微积分学

[如何理解导数：商规则、指数和对数](https://betterexplained.com/articles/how-to-understand-derivatives-the-quotient-rule-exponents-and-logarithms/)(betterexplained.com)

[如何理解衍生品：产品、权力和链规则](https://betterexplained.com/articles/derivatives-product-power-chain/)（betterexplained.com）

[向量微积分：理解梯度](https://betterexplained.com/articles/vector-calculus-understanding-the-gradient/)（betterexplained.com）

[微积分](http://web.stanford.edu/class/cs224n/lecture_notes/cs224n-2017-review-differential-calculus.pdf)（斯坦福 CS224n）

[微积分概述](http://ml-cheatsheet.readthedocs.io/en/latest/calculus.html)(readthedocs.io)
